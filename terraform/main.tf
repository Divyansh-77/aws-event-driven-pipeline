#
# Defines the core AWS infrastructure for an event-driven, serverless data pipeline.
# This configuration is fully automated and managed by Terraform.
#

# Appending a random suffix to all globally unique resources to prevent naming conflicts.
resource "random_id" "suffix" {
  byte_length = 4
}

# S3 bucket designed to receive and store raw, incoming sales data files.
resource "aws_s3_bucket" "data_bucket" {
  bucket = "sales-pipeline-data-${random_id.suffix.hex}"
}

# Creates a placeholder object to represent the 'sales_data/' directory upon creation.
# This ensures the S3 trigger prefix path exists automatically.
resource "aws_s3_object" "sales_data_folder" {
  bucket = aws_s3_bucket.data_bucket.id
  key    = "sales_data/"
}

# S3 bucket for storing the final, aggregated daily reports generated by the pipeline.
resource "aws_s3_bucket" "report_bucket" {
  bucket = "sales-pipeline-reports-${random_id.suffix.hex}"
}

# DynamoDB table for persisting the structured data after it's processed by the Lambda.
# Configured for On-Demand capacity to align with serverless, pay-per-use principles.
resource "aws_dynamodb_table" "sales_data_table" {
  name           = "sales-pipeline-data-table-${random_id.suffix.hex}"
  billing_mode   = "PAY_PER_REQUEST"
  # IMPORTANT: The hash_key must be a column name present in your sales_data.csv file.
  # Let's assume your CSV has a column named "transaction_id". If not, change this.
  hash_key       = "transaction_id"
  attribute {
    name = "transaction_id"
    type = "S" # "S" for String. Change to "N" if it's a number.
  }
}

# --- IAM Configuration for Data Processor ---

# IAM role that grants the Data Processor Lambda function the necessary permissions to execute.
resource "aws_iam_role" "data_processor_lambda_role" {
  name = "data-processor-role-${random_id.suffix.hex}"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Action = "sts:AssumeRole",
        Effect = "Allow",
        Principal = {
          Service = "lambda.amazonaws.com"
        }
      }
    ]
  })
}

# Defines the specific permissions (policy) for the Data Processor Lambda.
resource "aws_iam_policy" "data_processor_lambda_policy" {
  name   = "data-processor-policy-${random_id.suffix.hex}"
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      # Allows the Lambda to read files uploaded to the data S3 bucket.
      {
        Action = [
          "s3:GetObject"
        ],
        Effect   = "Allow",
        Resource = "${aws_s3_bucket.data_bucket.arn}/*"
      },
      # Grants permission to write items in batches, which is more efficient for the Python script.
      {
        Action = [
          "dynamodb:PutItem",
          "dynamodb:BatchWriteItem"
        ],
        Effect   = "Allow",
        Resource = aws_dynamodb_table.sales_data_table.arn
      },
      # Standard permissions for Lambda to write its execution logs to CloudWatch.
      {
        Action = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents"
        ],
        Effect   = "Allow",
        Resource = "arn:aws:logs:*:*:*"
      }
    ]
  })
}

# Links the defined policy to the Lambda role.
resource "aws_iam_role_policy_attachment" "data_processor_attach" {
  role       = aws_iam_role.data_processor_lambda_role.name
  policy_arn = aws_iam_policy.data_processor_lambda_policy.arn
}

# --- IAM Configuration for Report Generator ---

# IAM role for the scheduled, daily report generation Lambda.
resource "aws_iam_role" "report_generator_lambda_role" {
  name = "report-generator-role-${random_id.suffix.hex}"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Action = "sts:AssumeRole",
        Effect = "Allow",
        Principal = {
          Service = "lambda.amazonaws.com"
        }
      }
    ]
  })
}

# Defines permissions for the Report Generator Lambda.
resource "aws_iam_policy" "report_generator_lambda_policy" {
  name   = "report-generator-policy-${random_id.suffix.hex}"
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      # Allows the Lambda to read all items from the DynamoDB table to create a report.
      {
        Action = [
          "dynamodb:Scan"
        ],
        Effect   = "Allow",
        Resource = aws_dynamodb_table.sales_data_table.arn
      },
      # Allows the Lambda to save the final report file to the reports S3 bucket.
      {
        Action = [
          "s3:PutObject"
        ],
        Effect   = "Allow",
        Resource = "${aws_s3_bucket.report_bucket.arn}/*"
      },
      # Standard permissions for writing execution logs.
      {
        Action = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents"
        ],
        Effect   = "Allow",
        Resource = "arn:aws:logs:*:*:*"
      }
    ]
  })
}

# Links the policy to the report generator role.
resource "aws_iam_role_policy_attachment" "report_generator_attach" {
  role       = aws_iam_role.report_generator_lambda_role.name
  policy_arn = aws_iam_policy.report_generator_lambda_policy.arn
}

# --- Lambda Functions and Triggers ---

# Deploys the Data Processor Lambda function using the ZIP package created by the CI/CD pipeline.
resource "aws_lambda_function" "data_processor_lambda" {
  filename         = "../data_processor.zip"
  function_name    = "data-processor-lambda-${random_id.suffix.hex}"
  role             = aws_iam_role.data_processor_lambda_role.arn
  handler          = "lambda_function.lambda_handler"
  runtime          = "python3.9"
  source_code_hash = filebase64sha256("../data_processor.zip")

  environment {
    variables = {
      # This passes the DynamoDB table name to the Lambda function
      PROCESSED_DATA_TABLE_NAME = aws_dynamodb_table.sales_data_table.name
    }
  }
}

# Grants the S3 service the necessary permissions to invoke our Lambda function upon file upload.
resource "aws_lambda_permission" "allow_s3_to_call_lambda" {
  statement_id  = "AllowS3Invoke"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.data_processor_lambda.function_name
  principal     = "s3.amazonaws.com"
  source_arn    = aws_s3_bucket.data_bucket.arn
}

# This resource is the core of the event-driven architecture.
# It configures the S3 bucket to send a notification to Lambda when a new CSV file is created.
resource "aws_s3_bucket_notification" "bucket_notification" {
  bucket = aws_s3_bucket.data_bucket.id

  lambda_function {
    lambda_function_arn = aws_lambda_function.data_processor_lambda.arn
    events              = ["s3:ObjectCreated:*"]
    filter_prefix       = "sales_data/"
    filter_suffix       = ".csv"
  }

  depends_on = [aws_lambda_permission.allow_s3_to_call_lambda]
}

# Deploys the daily Report Generator Lambda function.
resource "aws_lambda_function" "report_generator" {
  filename         = "../report_generator.zip"
  function_name    = "report-generator-lambda-${random_id.suffix.hex}"
  role             = aws_iam_role.report_generator_lambda_role.arn
  handler          = "lambda_function.lambda_handler"
  runtime          = "python3.9"
  source_code_hash = filebase64sha256("../report_generator.zip")

  environment {
    variables = {
      TABLE_NAME     = aws_dynamodb_table.sales_data_table.name
      REPORTS_BUCKET = aws_s3_bucket.report_bucket.bucket
    }
  }
}

# EventBridge (CloudWatch Events) rule to trigger the report generator on a fixed schedule.
resource "aws_cloudwatch_event_rule" "daily_report_schedule" {
  name                = "daily-report-trigger-${random_id.suffix.hex}"
  description         = "Fires daily at midnight UTC to generate the sales report."
  schedule_expression = "cron(0 0 * * ? *)"
}

# Grants EventBridge the permission to invoke the report generator Lambda.
resource "aws_lambda_permission" "allow_cloudwatch_to_call_lambda" {
  statement_id  = "AllowCloudwatchInvoke"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.report_generator.function_name
  principal     = "events.amazonaws.com"
  source_arn    = aws_cloudwatch_event_rule.daily_report_schedule.arn
}

# Establishes the Lambda function as the target for the EventBridge rule.
resource "aws_cloudwatch_event_target" "trigger_lambda" {
  rule      = aws_cloudwatch_event_rule.daily_report_schedule.name
  target_id = "TriggerReportGeneratorLambda"
  arn       = aws_lambda_function.report_generator.arn
}

